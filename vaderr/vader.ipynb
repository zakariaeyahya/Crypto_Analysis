{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T18:57:17.778343Z","iopub.execute_input":"2025-11-28T18:57:17.779057Z","iopub.status.idle":"2025-11-28T18:57:18.039740Z","shell.execute_reply.started":"2025-11-28T18:57:17.779027Z","shell.execute_reply":"2025-11-28T18:57:18.039076Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/bitcoin-tweets-16m-tweets-with-sentiment-tagged/mbsa.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# 1. INSTALLATION AND IMPORTS\n\n# tqdm is useful for progress bars, which is essential for a large dataset\n!pip install tqdm -q\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport logging\nfrom tqdm.auto import tqdm\n\n# For splitting the data\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n\n# For VADER sentiment analysis\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S'\n)\n\n# Download necessary NLTK resources for VADER\nnltk.download('vader_lexicon', quiet=True)\n\n# Initialize tqdm for pandas to see progress on .apply() operations\ntqdm.pandas()\n\nlogging.info(\"Libraries imported and ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T18:58:24.399509Z","iopub.execute_input":"2025-11-28T18:58:24.400270Z","iopub.status.idle":"2025-11-28T18:58:29.991414Z","shell.execute_reply.started":"2025-11-28T18:58:24.400243Z","shell.execute_reply":"2025-11-28T18:58:29.990572Z"}},"outputs":[{"name":"stderr","text":"2025-11-28 18:58:29 - INFO - Libraries imported and ready.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 2. LOAD AND EXPLORE DATA\nimport logging\nimport pandas as pd\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S'\n)\n\nlogging.info(\"--- Step 2: Loading and Exploring Data ---\")\n\n# The path to the file in the Kaggle environment\nfile_path = '/kaggle/input/bitcoin-tweets-16m-tweets-with-sentiment-tagged/mbsa.csv'\n\n# For development, it is HIGHLY recommended to load only a sample.\n# Once the code is confirmed to work, remove 'nrows' to load all 16M tweets.\ndf = pd.read_csv(file_path, nrows=500000) # Tip: start with 500k rows\n\nlogging.info(f\"Number of tweets loaded: {len(df)}\")\nlogging.info(f\"Available columns: {df.columns.tolist()}\")\n\nlogging.info(\"First 5 rows preview:\")\n# We convert the dataframe head to string to log it properly\nlogging.info(\"\\n\" + str(df.head()))\n\nlogging.info(\"Sentiment distribution:\")\nlogging.info(\"\\n\" + str(df['Sentiment'].value_counts(normalize=True)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T18:59:17.723828Z","iopub.execute_input":"2025-11-28T18:59:17.724485Z","iopub.status.idle":"2025-11-28T18:59:20.183723Z","shell.execute_reply.started":"2025-11-28T18:59:17.724454Z","shell.execute_reply":"2025-11-28T18:59:20.182979Z"}},"outputs":[{"name":"stderr","text":"2025-11-28 18:59:17 - INFO - --- Step 2: Loading and Exploring Data ---\n2025-11-28 18:59:20 - INFO - Number of tweets loaded: 500000\n2025-11-28 18:59:20 - INFO - Available columns: ['Date', 'text', 'Sentiment']\n2025-11-28 18:59:20 - INFO - First 5 rows preview:\n2025-11-28 18:59:20 - INFO - \n         Date                                               text Sentiment\n0  2019-05-27  È appena uscito un nuovo video! LES CRYPTOMONN...  Positive\n1  2019-05-27  Cardano: Digitize Currencies; EOS https://t.co...  Positive\n2  2019-05-27  Another Test tweet that wasn't caught in the s...  Positive\n3  2019-05-27  Current Crypto Prices! \\n\\nBTC: $8721.99 USD\\n...  Positive\n4  2019-05-27  Spiv (Nosar Baz): BITCOIN Is An Asset &amp; NO...  Positive\n2025-11-28 18:59:20 - INFO - Sentiment distribution:\n2025-11-28 18:59:20 - INFO - \nSentiment\nPositive    0.675518\nNegative    0.324482\nName: proportion, dtype: float64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# 3. CLEAN AND PREPARE DATA\nimport logging\nimport re\n\nlogging.info(\"--- Step 3: Cleaning and Preparing Data ---\")\n\ndef clean_tweet(text):\n    \"\"\"\n    Function to clean the text of a tweet.\n    - Removes URLs\n    - Removes mentions (@username)\n    - Removes hashtags (#hashtag)\n    - Removes non-alphanumeric characters (except spaces)\n    - Converts to lowercase\n    - Removes extra whitespace\n    \"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text) # Remove URLs\n    text = re.sub(r'@\\w+', '', text)                 # Remove mentions\n    text = re.sub(r'#\\w+', '', text)                 # Remove hashtags\n    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)       # Remove special characters\n    text = text.lower()                              # Convert to lowercase\n    text = re.sub(r'\\s+', ' ', text).strip()         # Remove extra whitespace\n    return text\n\n# Apply the cleaning function with a progress bar\n# Ensure tqdm.pandas() was run in Step 1\ndf['cleaned_text'] = df['text'].progress_apply(clean_tweet)\n\n# Drop tweets that are empty after cleaning\ndf.dropna(subset=['cleaned_text'], inplace=True)\ndf = df[df['cleaned_text'] != '']\n\n# Map sentiments to numerical values for models\n# Positive -> 1, Negative -> 0. We ignore others for this binary case.\ndf = df[df['Sentiment'].isin(['Positive', 'Negative'])]\ndf['label'] = df['Sentiment'].map({'Positive': 1, 'Negative': 0})\n\nlogging.info(\"Preview after cleaning and preparation:\")\nlogging.info(\"\\n\" + str(df[['cleaned_text', 'Sentiment', 'label']].head()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T19:00:11.301630Z","iopub.execute_input":"2025-11-28T19:00:11.301909Z","iopub.status.idle":"2025-11-28T19:00:19.277666Z","shell.execute_reply.started":"2025-11-28T19:00:11.301882Z","shell.execute_reply":"2025-11-28T19:00:19.277069Z"}},"outputs":[{"name":"stderr","text":"2025-11-28 19:00:11 - INFO - --- Step 3: Cleaning and Preparing Data ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/500000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6c5146786094662b22fa89a78b75640"}},"metadata":{}},{"name":"stderr","text":"2025-11-28 19:00:19 - INFO - Preview after cleaning and preparation:\n2025-11-28 19:00:19 - INFO - \n                                        cleaned_text Sentiment  label\n0  appena uscito un nuovo video les cryptomonnaie...  Positive      1\n1  cardano digitize currencies eos 6500 roi atamp...  Positive      1\n2  another test tweet that wasnt caught in the st...  Positive      1\n3  current crypto prices btc 872199 usd eth 26662...  Positive      1\n4  spiv nosar baz bitcoin is an asset amp not a c...  Positive      1\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# 4. CREATE DATA SPLITS (TRAIN/VALIDATION/TEST)\nimport logging\nfrom sklearn.model_selection import train_test_split\n\nlogging.info(\"--- Step 4: Creating Data Splits ---\")\n\nX = df['cleaned_text']\ny = df['label']\n\n# Step A: Split into Train (70%) and a temporary set (30%)\nX_train, X_temp, y_train, y_temp = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\n# Step B: Split the temporary set into Validation (15%) and Test (15%)\n# (0.5 * 30% = 15%)\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n)\n\nlogging.info(f\"Training set size   : {len(X_train)} ({len(X_train)/len(df):.0%})\")\nlogging.info(f\"Validation set size : {len(X_val)} ({len(X_val)/len(df):.0%})\")\nlogging.info(f\"Test set size       : {len(X_test)} ({len(X_test)/len(df):.0%})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T19:01:16.897239Z","iopub.execute_input":"2025-11-28T19:01:16.898098Z","iopub.status.idle":"2025-11-28T19:01:17.136043Z","shell.execute_reply.started":"2025-11-28T19:01:16.898061Z","shell.execute_reply":"2025-11-28T19:01:17.135492Z"}},"outputs":[{"name":"stderr","text":"2025-11-28 19:01:16 - INFO - --- Step 4: Creating Data Splits ---\n2025-11-28 19:01:17 - INFO - Training set size   : 323470 (70%)\n2025-11-28 19:01:17 - INFO - Validation set size : 69315 (15%)\n2025-11-28 19:01:17 - INFO - Test set size       : 69316 (15%)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport os\nimport logging\nfrom tqdm import tqdm\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S'\n)\n\n# Configuration\ncsv_file = '/kaggle/input/bitcoin-tweets-16m-tweets-with-sentiment-tagged/mbsa.csv'\nchunk_size = 100000 \nprobs = [0.70, 0.15, 0.15] # 70% Train, 15% Val, 15% Test\n\n# Output files\ntrain_file = 'train_full.csv'\nval_file = 'val_full.csv'\ntest_file = 'test_full.csv'\n\n# Delete existing files to start fresh\nfor f in [train_file, val_file, test_file]:\n    if os.path.exists(f):\n        os.remove(f)\n        logging.info(f\"Removed existing file: {f}\")\n\nlogging.info(\"Generating files (1M sample for speed)...\")\n\n# Text cleaning function\ndef clean_text(text):\n    text = str(text).lower()\n    text = re.sub(r'http\\S+', '', text)    # Remove URLs\n    text = re.sub(r'[^\\w\\s]', '', text)    # Remove punctuation\n    return text.strip()\n\n# Label mapping\nlabel_map = {'negative': 0, 'Negative': 0, 'neutral': 1, 'Neutral': 1, 'positive': 2, 'Positive': 2}\n\n# Data reading (Limited to 1M rows for this run)\nreader = pd.read_csv(csv_file, chunksize=chunk_size, nrows=1000000,\n                     on_bad_lines='skip', engine='python',\n                     usecols=lambda c: c.lower() in ['text', 'sentiment'])\n\nfor i, chunk in enumerate(tqdm(reader, desc=\"Processing\")):\n    chunk = chunk.copy()\n    \n    # Standardization\n    chunk.columns = [c.lower() for c in chunk.columns]\n    chunk.dropna(inplace=True)\n    \n    # Cleaning\n    chunk['cleaned_text'] = chunk['text'].apply(clean_text)\n    chunk = chunk[chunk['cleaned_text'] != \"\"]\n    \n    # Label Encoding\n    chunk['label'] = chunk['sentiment'].map(label_map)\n    chunk = chunk.dropna(subset=['label'])\n    chunk['label'] = chunk['label'].astype(int)\n    \n    # Train/Val/Test Split\n    split = np.random.choice(['train', 'val', 'test'], size=len(chunk), p=probs)\n    \n    # Saving to CSV\n    mode = 'a' if i > 0 else 'w'\n    header = (i == 0)\n    \n    chunk[split == 'train'][['cleaned_text', 'label']].to_csv(train_file, mode=mode, header=header, index=False)\n    chunk[split == 'val'][['cleaned_text', 'label']].to_csv(val_file, mode=mode, header=header, index=False)\n    chunk[split == 'test'][['cleaned_text', 'label']].to_csv(test_file, mode=mode, header=header, index=False)\n\nlogging.info(\"Files regenerated! You can restart VADER training.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T19:01:41.116581Z","iopub.execute_input":"2025-11-28T19:01:41.117271Z","iopub.status.idle":"2025-11-28T19:01:57.913520Z","shell.execute_reply.started":"2025-11-28T19:01:41.117244Z","shell.execute_reply":"2025-11-28T19:01:57.912938Z"}},"outputs":[{"name":"stderr","text":"2025-11-28 19:01:41 - INFO - Generating files (1M sample for speed)...\nProcessing: 10it [00:16,  1.68s/it]\n2025-11-28 19:01:57 - INFO - Files regenerated! You can restart VADER training.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\nimport json\nimport os\nimport logging\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nimport nltk\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S'\n)\n\n# Config\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\nlogging.info(\"Starting vocabulary learning...\")\n\n# 1. Load training data (1 Million rows is sufficient for learning)\nif os.path.exists('train_full.csv'):\n    # Read only useful columns\n    df_train = pd.read_csv('train_full.csv', nrows=1000000)\n    \n    # 2. Clean labels (Safety: ensure we have 0 and 1)\n    def clean_label(x):\n        # If it's 2, 'Positive', or 'positive' -> convert to 1\n        if str(x) in ['2', 'Positive', 'positive', '1']: return 1\n        return 0 # Otherwise 0\n    \n    df_train['label_bin'] = df_train['label'].apply(clean_label)\n    \n    logging.info(\"Analyzing word frequencies (approx 30 sec)...\")\n    \n    # 3. Separate texts\n    # Convert everything to string to avoid bugs\n    pos_text = ' '.join(df_train[df_train['label_bin'] == 1]['cleaned_text'].astype(str))\n    neg_text = ' '.join(df_train[df_train['label_bin'] == 0]['cleaned_text'].astype(str))\n    \n    # 4. Counting\n    pos_counts = Counter(pos_text.split())\n    neg_counts = Counter(neg_text.split())\n    \n    # 5. Calculate Score for each word\n    new_lexicon = {}\n    all_words = set(pos_counts.keys()).union(set(neg_counts.keys()))\n    \n    logging.info(f\"Calculating scores for {len(all_words)} unique words...\")\n    \n    for word in all_words:\n        # Ignore short words or stop words\n        if len(word) < 3 or word in stop_words: continue\n        \n        p = pos_counts[word]\n        n = neg_counts[word]\n        total = p + n\n        \n        # Word must appear at least 50 times to be reliable\n        if total < 50: continue\n        \n        ratio = p / total\n        \n        score = 0\n        # If the word appears > 75% in positive tweets -> Positive Score\n        if ratio > 0.75: \n            score = 2.0 + (ratio * 2.0) # Max 4.0\n        # If the word appears < 25% (mostly negative) -> Negative Score\n        elif ratio < 0.25:\n            score = -2.0 - ((1-ratio) * 2.0) # Max -4.0\n            \n        if score != 0:\n            new_lexicon[word] = round(score, 2)\n\n    # 6. Save\n    with open('crypto_lexicon_final.json', 'w') as f:\n        json.dump(new_lexicon, f)\n        \n    logging.info(f\"SUCCESS: File 'crypto_lexicon_final.json' created with {len(new_lexicon)} words.\")\n    logging.info(\"You can now run Step 2!\")\n\nelse:\n    logging.error(\"ERROR: Could not find 'train_full.csv'. Did you run the Split step?\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T19:02:34.877256Z","iopub.execute_input":"2025-11-28T19:02:34.877533Z","iopub.status.idle":"2025-11-28T19:02:40.615464Z","shell.execute_reply.started":"2025-11-28T19:02:34.877515Z","shell.execute_reply":"2025-11-28T19:02:40.614753Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n2025-11-28 19:02:34 - INFO - Starting vocabulary learning...\n2025-11-28 19:02:36 - INFO - Analyzing word frequencies (approx 30 sec)...\n2025-11-28 19:02:40 - INFO - Calculating scores for 562587 unique words...\n2025-11-28 19:02:40 - INFO - SUCCESS: File 'crypto_lexicon_final.json' created with 4557 words.\n2025-11-28 19:02:40 - INFO - You can now run Step 2!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\nimport json\nimport nltk\nimport logging\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom tqdm import tqdm\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S'\n)\n\n# Config\nnltk.download('vader_lexicon')\nsid = SentimentIntensityAnalyzer()\n\nlogging.info(\"--- FINAL VADER EVALUATION (HYBRID) ---\")\n\n# 1. Load Automatic Lexicon (the one created in the previous step)\ntry:\n    with open('crypto_lexicon_final.json', 'r') as f:\n        auto_lexicon = json.load(f)\n    logging.info(f\"Automatic Lexicon loaded: {len(auto_lexicon)} words.\")\nexcept FileNotFoundError:\n    logging.warning(\"ERROR: JSON file not found. Please rerun Step 1!\")\n    auto_lexicon = {}\n\n# 2. Manual Lexicon (Priority)\nmanual_lexicon = {\n    'hodl': 3.5, 'moon': 4.0, 'bullish': 3.5, 'ath': 3.5, 'pump': 2.5,\n    'bearish': -3.5, 'fud': -3.0, 'dump': -3.5, 'scam': -4.0, 'rekt': -4.0,\n    'btc': 1.0, 'bitcoin': 1.0, 'buy': 2.5, 'long': 2.5, 'short': -2.5, 'sell': -2.5\n}\n\n# 3. Merge and Update VADER\nsid.lexicon.update(auto_lexicon)    # AI/Statistical lexicon first\nsid.lexicon.update(manual_lexicon)  # Manual lexicon second (overwrites duplicates)\nlogging.info(\"VADER updated and ready.\")\n\n# 4. Prediction on Test Set\nlogging.info(\"Loading Test Set...\")\ntest_df = pd.read_csv('test_full.csv') \n\n# Fix labels (Ensure binary 0/1)\ntest_df['label_bin'] = test_df['label'].apply(lambda x: 1 if str(x) in ['2', 'Positive', 'positive', '1'] else 0)\n\n# Prediction function\npredictions = []\nthreshold = 0.0 # Strict threshold for binary classification\n\nlogging.info(\"Predictions in progress...\")\nfor text in tqdm(test_df['cleaned_text'].astype(str)):\n    try:\n        score = sid.polarity_scores(text)['compound']\n        predictions.append(1 if score > threshold else 0)\n    except:\n        predictions.append(0)\n\n# 5. Results\nacc = accuracy_score(test_df['label_bin'], predictions)\nlogging.info(\"=\"*40)\nlogging.info(f\"FINAL VADER SCORE: {acc:.2%}\")\nlogging.info(\"=\"*40)\n\nlogging.info(\"Confusion Matrix:\")\n# Logging the matrix as a string to preserve structure\nlogging.info(\"\\n\" + str(confusion_matrix(test_df['label_bin'], predictions)))\n\nlogging.info(\"Detailed Report:\")\nlogging.info(\"\\n\" + classification_report(test_df['label_bin'], predictions, target_names=['Negative', 'Positive']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T19:02:59.836342Z","iopub.execute_input":"2025-11-28T19:02:59.836941Z","iopub.status.idle":"2025-11-28T19:03:27.384570Z","shell.execute_reply.started":"2025-11-28T19:02:59.836914Z","shell.execute_reply":"2025-11-28T19:03:27.384001Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n2025-11-28 19:02:59 - INFO - --- FINAL VADER EVALUATION (HYBRID) ---\n2025-11-28 19:02:59 - INFO - Automatic Lexicon loaded: 4557 words.\n2025-11-28 19:02:59 - INFO - VADER updated and ready.\n2025-11-28 19:02:59 - INFO - Loading Test Set...\n2025-11-28 19:03:00 - INFO - Predictions in progress...\n100%|██████████| 148664/148664 [00:26<00:00, 5513.70it/s]\n2025-11-28 19:03:27 - INFO - ========================================\n2025-11-28 19:03:27 - INFO - FINAL VADER SCORE: 69.80%\n2025-11-28 19:03:27 - INFO - ========================================\n2025-11-28 19:03:27 - INFO - Confusion Matrix:\n2025-11-28 19:03:27 - INFO - \n[[ 8058 33989]\n [10904 95713]]\n2025-11-28 19:03:27 - INFO - Detailed Report:\n2025-11-28 19:03:27 - INFO - \n              precision    recall  f1-score   support\n\n    Negative       0.42      0.19      0.26     42047\n    Positive       0.74      0.90      0.81    106617\n\n    accuracy                           0.70    148664\n   macro avg       0.58      0.54      0.54    148664\nweighted avg       0.65      0.70      0.66    148664\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}