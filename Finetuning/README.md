# RoBERTa Fine-tuning for Crypto Tweet Sentiment Analysis

This project implements fine-tuning of the RoBERTa model to classify Bitcoin tweet sentiment using a dataset of 19+ million tweets.

## Table of Contents

- [Overview](#overview)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Configuration](#configuration)
- [Usage](#usage)
- [Results](#results)
- [Documentation](#documentation)

## Overview

The project fine-tunes the **RoBERTa** model (specifically `cardiffnlp/twitter-roberta-base-sentiment`) to classify crypto tweet sentiment into three categories:
- **POSITIVE**: Positive sentiment
- **NEGATIVE**: Negative sentiment  
- **NEUTRAL**: Neutral sentiment

### Key Features

- CUDA/GPU support for accelerated training
- Mixed precision training (FP16) to optimize memory
- Large dataset handling (chunk-based loading)
- Early stopping to prevent overfitting
- Automatic checkpoints
- Complete results visualization
- Inference script for new predictions

### Code Format

This project is available in **two equivalent formats**:

1. **Python files (.py)**: Modular code organized in separate scripts
   - `data_preparation.py`, `preprocessing.py`, `train.py`, `evaluate.py`, etc.
   - Ideal for command-line execution and pipeline integration

2. **Jupyter Notebook (.ipynb)**: `roberta-1-1.ipynb`
   - Contains **exactly the same code** as the .py files
   - Organized in cells for interactive exploration
   - Ideal for development, debugging, and step-by-step visualization

**Important note**: The `roberta-1-1.ipynb` notebook and `.py` files implement the same logic and produce the same results. Training results generated by the notebook are available in the `output/` directory. For detailed results interpretation, see `output/results/README_INTERPRETATION.md`.

## Project Structure

```
Finetuning/
├── config.yaml              # Global configuration
├── data_preparation.py      # Data loading and analysis
├── preprocessing.py         # Text cleaning and preprocessing
├── model_config.py          # RoBERTa model configuration
├── train.py                 # Main training script
├── evaluate.py              # Model evaluation
├── inference.py             # Predictions on new tweets
├── visualize.py             # Results visualization
├── utils.py                 # Utility functions
├── requirements.txt         # Python dependencies
├── README.md                # Documentation
├── roberta-1-1.ipynb        # Jupyter notebook (same code as .py files)
├── roberta_finetuning_prompt.txt  # Original project prompt
├── models/                  # Saved models
├── logs/                    # Training logs
├── checkpoints/             # Intermediate checkpoints
├── results/                 # Results and visualizations
└── output/                  # Results generated by notebook
    ├── models/              # Trained models
    │   ├── best_model/      # Best model
    │   ├── final_model/     # Final model
    │   └── label_mapping.json
    └── results/              # Metrics and visualizations
        ├── training_metrics.json
        ├── training_curves.png
        ├── confusion_matrix.png
        ├── train_split.csv
        ├── val_split.csv
        ├── test_split.csv
        └── README_INTERPRETATION.md  # Detailed results interpretation
```

## Installation

### Prerequisites

- Python 3.8+
- CUDA-capable GPU (recommended, 8GB+ VRAM)
- 10GB+ disk space for the dataset

### Installing Dependencies

```bash
pip install -r Finetuning/requirements.txt
```

### Installing PyTorch with CUDA

To use GPU, install PyTorch with CUDA support:

```bash
# For CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# For CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

Verify CUDA is available:

```python
import torch
print(torch.cuda.is_available())  # Should return True
print(torch.cuda.get_device_name(0))  # GPU name
```

## Configuration

The `config.yaml` file contains all project configuration. Main sections:

### Model
- `model.name`: Hugging Face model to use
- `model.num_labels`: Number of classes (3 for sentiment)
- `model.max_length`: Maximum sequence length (128 tokens)

### Data
- `data.csv_path`: Path to CSV dataset
- `data.sample_size`: Number of tweets to use (None = all)
- `data.train_split`, `data.val_split`, `data.test_split`: Split proportions

### Training
- `training.batch_size`: Batch size (32 by default)
- `training.learning_rate`: Learning rate (2e-5 by default)
- `training.num_epochs`: Number of epochs (5 by default)
- `training.use_fp16`: Mixed precision training (True by default)
- `training.use_cuda`: Use CUDA if available (True by default)

## Usage

### 1. Data Preparation

The dataset must be in `data/bronze/kaggle/bitcoin_tweets_YYYYMMDD_HHMMSS.csv` with columns:
- `Date`: Tweet date
- `text`: Tweet text
- `Sentiment`: Label (POSITIVE/NEGATIVE/NEUTRAL)

### 2. Training

Launch training:

```bash
python Finetuning/train.py
```

The script will:
1. Load and analyze data
2. Preprocess texts
3. Split into train/val/test
4. Train model with CUDA
5. Save checkpoints and final model

### 3. Evaluation

Evaluate model on test set:

```bash
python Finetuning/evaluate.py
```

This generates:
- Detailed metrics (accuracy, F1, precision, recall)
- Confusion matrix
- Classification report
- JSON file with all results

### 4. Visualizations

Create visualizations:

```bash
python Finetuning/visualize.py
```

Generates:
- Training curves (loss, accuracy, F1)
- Confusion matrix
- Metrics comparison by class

### 5. Inference

Predict sentiment of new tweets:

```python
from Finetuning.inference import SentimentPredictor

# Load model
predictor = SentimentPredictor(
    model_path="Finetuning/models/roberta-bitcoin-sentiment",
    use_cuda=True
)

# Simple prediction
sentiment = predictor.predict("Bitcoin is going to the moon!")
print(sentiment)  # "POSITIVE"

# Prediction with probabilities
result = predictor.predict("Bitcoin is going to the moon!", return_proba=True)
print(result)
# {
#     "label": "POSITIVE",
#     "probabilities": {"POSITIVE": 0.85, "NEUTRAL": 0.10, "NEGATIVE": 0.05},
#     "confidence": 0.85
# }

# Batch prediction
tweets = ["Tweet 1", "Tweet 2", "Tweet 3"]
predictions = predictor.predict_batch(tweets, batch_size=32)
```

## Results

### Target Metrics

- Accuracy > 75%
- F1-Score macro > 0.70
- F1-Score per class > 0.65
- Inference time < 100ms per tweet

### Training Results

Training results from the `roberta-1-1.ipynb` notebook are available in `Finetuning/output/`:

**Obtained metrics**:
- **Validation Accuracy**: 70.1%
- **Validation F1-Score macro**: 0.686 (68.6%)
- **Best F1-Score**: 0.686 (epoch 8)
- **Training time**: ~8.8 hours (8 epochs)

For **detailed interpretation** of results, see:
- `output/results/README_INTERPRETATION.md`: Complete performance analysis, metrics evolution, improvement recommendations

### Generated Files

After training, you will find in `Finetuning/results/` (or `Finetuning/output/results/` for notebook):

- `training_metrics.json`: Training metrics
- `evaluation_results.json`: Evaluation results
- `training_curves.png`: Training curves
- `confusion_matrix.png`: Confusion matrix
- `metrics_comparison.png`: Metrics comparison

In `Finetuning/models/` (or `Finetuning/output/models/` for notebook):

- `best_model/`: Best model based on validation F1-score
- `final_model/`: Final model after all epochs
- `label_mapping.json`: Label mapping

In `Finetuning/checkpoints/`:

- `checkpoint_epoch_N.pt`: Checkpoints per epoch
- `best_model.pt`: Best model (based on validation F1)

## Optimizations

### For GPU with Limited Memory

1. Reduce `batch_size` (16 or 8)
2. Enable `gradient_accumulation_steps` (2 or 4)
3. Use `use_fp16: true` for mixed precision
4. Reduce `max_length` (64 or 96 instead of 128)

### For Very Large Datasets

1. Use `sample_size` to sample
2. Increase `chunk_size` for loading
3. Use data streaming (to be implemented)

## Troubleshooting

### CUDA Not Available

If `torch.cuda.is_available()` returns `False`:

1. Check PyTorch installation with CUDA
2. Check that NVIDIA drivers are up to date
3. Code will automatically fall back to CPU

### GPU Memory Error

1. Reduce `batch_size`
2. Enable `use_fp16: true`
3. Reduce `sample_size` to use less data

### Dataset Too Large

1. Use `sample_size` in config (e.g., 1000000)
2. Increase `chunk_size` for loading

## Documentation

- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [RoBERTa Paper](https://arxiv.org/abs/1907.11692)
- [PyTorch Documentation](https://pytorch.org/docs/)

## Notes

- Complete dataset is ~3GB with 19M tweets
- Training on complete dataset can take several hours/days
- Start with a sample (100k-1M tweets) to test
- Model uses `cardiffnlp/twitter-roberta-base-sentiment` which is pre-trained on Twitter

## Contribution

To improve the project:
1. Test different hyperparameters
2. Try other pre-trained models
3. Implement data augmentation
4. Add cross-validation

## License

This project is part of the Crypto_Analysis project.
