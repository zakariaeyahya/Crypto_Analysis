# Configuration pour le fine-tuning de RoBERTa sur sentiment analysis
model:
  name: "cardiffnlp/twitter-roberta-base-sentiment"  # Modèle pré-entraîné sur Twitter
  num_labels: 3
  max_length: 64  # Réduit au minimum pour 2GB GPU
  freeze_layers: true  # Geler les premières couches pour économiser VRAM

data:
  csv_path: "../data/bronze/kaggle/bitcoin_tweets_20251119_165829.csv"
  sample_size: 1000000  # Nombre de tweets à utiliser (ajuster selon ressources)
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  stratify: true
  chunk_size: 10000  # Pour le chargement par chunks

preprocessing:
  remove_urls: true
  handle_mentions: true  # Garder @user
  handle_hashtags: true  # Garder texte, enlever #
  normalize_spaces: true
  handle_emojis: "keep"  # Options: keep, remove, convert

training:
  batch_size: 2  # Minimum pour GTX 1050 (2GB VRAM)
  learning_rate: 0.00002  # 2e-5 en notation décimale
  num_epochs: 5
  warmup_steps: 1000
  weight_decay: 0.01
  max_grad_norm: 1.0
  gradient_accumulation_steps: 16  # Simule batch_size=32 (2x16=32)
  use_fp16: true  # Mixed precision training
  early_stopping:
    enabled: true
    patience: 3
    monitor: "val_f1"
  
  # GPU/CUDA settings
  device: "cuda"  # Utiliser CUDA si disponible
  use_cuda: true

evaluation:
  metrics: ["accuracy", "f1", "precision", "recall"]
  save_confusion_matrix: true
  save_classification_report: true

paths:
  models_dir: "models"
  logs_dir: "logs"
  checkpoints_dir: "checkpoints"
  results_dir: "results"
  model_save_name: "roberta-bitcoin-sentiment"

logging:
  use_wandb: false  # Mettre à true si vous avez un compte wandb
  use_tensorboard: true
  log_interval: 100  # Logger toutes les N steps

reproducibility:
  seed: 42

