"""
Script de test pour v√©rifier que l'inf√©rence utilise le mod√®le du dossier output
"""
import sys
from pathlib import Path
import logging

# Ajouter le chemin du projet
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))
sys.path.insert(0, str(Path(__file__).resolve().parent))

logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

def test_inference_with_output_model():
    """Tester l'inf√©rence avec le mod√®le du dossier output"""
    
    print("=" * 70)
    print("TEST D'INF√âRENCE AVEC LE MOD√àLE DU DOSSIER OUTPUT")
    print("=" * 70)
    
    # Chemin vers le mod√®le dans output
    model_path = Path("output/models/best_model")
    
    # V√©rifier que le mod√®le existe
    print(f"\n1. V√©rification de l'existence du mod√®le...")
    print(f"   Chemin: {model_path.absolute()}")
    
    if not model_path.exists():
        print(f"   ‚ùå ERREUR: Le dossier {model_path} n'existe pas!")
        return False
    
    # V√©rifier les fichiers n√©cessaires
    required_files = ["config.json", "model.safetensors", "tokenizer_config.json"]
    print(f"\n2. V√©rification des fichiers du mod√®le...")
    all_files_exist = True
    for file in required_files:
        file_path = model_path / file
        exists = file_path.exists()
        status = "‚úÖ" if exists else "‚ùå"
        print(f"   {status} {file}: {'Existe' if exists else 'MANQUANT'}")
        if not exists:
            all_files_exist = False
    
    if not all_files_exist:
        print(f"   ‚ùå ERREUR: Certains fichiers sont manquants!")
        return False
    
    # V√©rifier le label mapping (il est dans le dossier parent models/)
    label_mapping_path = model_path.parent / "label_mapping.json"
    print(f"\n3. V√©rification du label mapping...")
    print(f"   Chemin: {label_mapping_path.absolute()}")
    if label_mapping_path.exists():
        import json
        with open(label_mapping_path, 'r') as f:
            label_mapping = json.load(f)
        print(f"   ‚úÖ Label mapping trouv√©: {label_mapping}")
    else:
        # Essayer aussi dans le dossier du mod√®le
        label_mapping_path2 = model_path / "label_mapping.json"
        if label_mapping_path2.exists():
            import json
            with open(label_mapping_path2, 'r') as f:
                label_mapping = json.load(f)
            print(f"   ‚úÖ Label mapping trouv√© dans le dossier du mod√®le: {label_mapping}")
        else:
            print(f"   ‚ö†Ô∏è  Label mapping non trouv√© (optionnel)")
    
    # Tester le chargement du mod√®le
    print(f"\n4. Test de chargement du mod√®le...")
    try:
        from transformers import RobertaForSequenceClassification, RobertaTokenizer
        
        print(f"   Chargement du tokenizer...")
        tokenizer = RobertaTokenizer.from_pretrained(str(model_path))
        print(f"   ‚úÖ Tokenizer charg√©")
        
        print(f"   Chargement du mod√®le...")
        model = RobertaForSequenceClassification.from_pretrained(str(model_path))
        print(f"   ‚úÖ Mod√®le charg√©")
        print(f"   Architecture: {model.config.architectures[0]}")
        print(f"   Nombre de labels: {model.config.num_labels if hasattr(model.config, 'num_labels') else 'N/A'}")
        
    except Exception as e:
        print(f"   ‚ùå ERREUR lors du chargement: {e}")
        return False
    
    # Tester l'inf√©rence avec le script
    print(f"\n5. Test d'inf√©rence avec le script inference.py...")
    try:
        from inference import SentimentPredictor
        
        print(f"   Initialisation du pr√©dicteur...")
        predictor = SentimentPredictor(
            model_path=str(model_path),
            use_cuda=False  # Utiliser CPU pour le test
        )
        print(f"   ‚úÖ Pr√©dicteur initialis√©")
        
        # Test avec quelques exemples
        test_tweets = [
            "Bitcoin is going to the moon! üöÄ",
            "I lost all my money in crypto",
            "Bitcoin price is stable today"
        ]
        
        print(f"\n6. Test de pr√©dictions...")
        for i, tweet in enumerate(test_tweets, 1):
            try:
                result = predictor.predict(tweet, return_proba=True)
                print(f"\n   Tweet {i}: {tweet}")
                print(f"   ‚Üí Sentiment: {result['label']}")
                print(f"   ‚Üí Confiance: {result['confidence']:.2%}")
                print(f"   ‚Üí Probabilit√©s: {result['probabilities']}")
            except Exception as e:
                print(f"   ‚ùå Erreur pour le tweet {i}: {e}")
                return False
        
        print(f"\n‚úÖ TOUS LES TESTS SONT PASS√âS!")
        print(f"\nLe mod√®le du dossier output fonctionne correctement avec inference.py")
        return True
        
    except Exception as e:
        print(f"   ‚ùå ERREUR lors de l'inf√©rence: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_inference_with_output_model()
    
    print("\n" + "=" * 70)
    if success:
        print("‚úÖ R√âSULTAT: Le script inference.py utilise bien le mod√®le du dossier output")
    else:
        print("‚ùå R√âSULTAT: Probl√®me d√©tect√© - voir les erreurs ci-dessus")
    print("=" * 70)
    
    sys.exit(0 if success else 1)

