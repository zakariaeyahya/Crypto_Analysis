# üìä Interpr√©tation des R√©sultats du Fine-tuning RoBERTa

Ce document pr√©sente une analyse d√©taill√©e des r√©sultats obtenus lors du fine-tuning du mod√®le RoBERTa pour l'analyse de sentiment des tweets Bitcoin.

## üìã Vue d'ensemble

- **Mod√®le** : RoBERTa (RobertaForSequenceClassification)
- **Dataset** : ~397,516 tweets Bitcoin
- **Classes** : 2 classes (Negative, Positive)
- **Nombre d'epochs** : 8
- **Temps d'entra√Ænement** : ~8.8 heures (31,722 secondes)

## üìà M√©triques d'entra√Ænement

### √âvolution de la Loss

#### Training Loss
La loss d'entra√Ænement diminue r√©guli√®rement de **0.538** (epoch 1) √† **0.292** (epoch 8), indiquant que le mod√®le apprend progressivement √† classifier les sentiments.

| Epoch | Training Loss |
|-------|---------------|
| 1     | 0.538         |
| 2     | 0.464         |
| 3     | 0.419         |
| 4     | 0.383         |
| 5     | 0.350         |
| 6     | 0.324         |
| 7     | 0.305         |
| 8     | 0.292         |

**Interpr√©tation** : La diminution constante de la loss d'entra√Ænement montre que le mod√®le continue d'apprendre. Cependant, la perte de validation augmente apr√®s l'epoch 2, ce qui sugg√®re un d√©but d'overfitting.

#### Validation Loss
La loss de validation pr√©sente une √©volution plus irr√©guli√®re :

| Epoch | Validation Loss |
|-------|-----------------|
| 1     | 0.668           |
| 2     | 0.712 ‚¨ÜÔ∏è        |
| 3     | 0.627 ‚¨áÔ∏è        |
| 4     | 0.698 ‚¨ÜÔ∏è        |
| 5     | 0.732 ‚¨ÜÔ∏è        |
| 6     | 0.738 ‚¨ÜÔ∏è        |
| 7     | 0.777 ‚¨ÜÔ∏è        |
| 8     | 0.770 ‚¨áÔ∏è        |

**Interpr√©tation** : 
- La validation loss augmente globalement apr√®s l'epoch 2, ce qui indique un **overfitting**.
- Le meilleur mod√®le aurait probablement √©t√© obtenu √† l'epoch 3 (validation loss = 0.627).
- L'√©cart croissant entre training loss et validation loss confirme l'overfitting.

### √âvolution de l'Accuracy

L'accuracy de validation s'am√©liore progressivement :

| Epoch | Validation Accuracy |
|-------|---------------------|
| 1     | 56.1%               |
| 2     | 62.0%               |
| 3     | 67.1%               |
| 4     | 67.4%               |
| 5     | 68.3%               |
| 6     | 69.0%               |
| 7     | 69.9%               |
| 8     | **70.1%**          |

**Interpr√©tation** :
- L'accuracy augmente de mani√®re constante, passant de 56.1% √† 70.1%.
- L'am√©lioration ralentit apr√®s l'epoch 3, sugg√©rant que le mod√®le atteint un plateau.
- L'accuracy finale de **70.1%** est acceptable mais pourrait √™tre am√©lior√©e.

### √âvolution du F1-Score

Le F1-score macro suit une √©volution similaire √† l'accuracy :

| Epoch | Validation F1-Score |
|-------|---------------------|
| 1     | 0.561               |
| 2     | 0.618               |
| 3     | 0.661               |
| 4     | 0.664               |
| 5     | 0.672               |
| 6     | 0.678               |
| 7     | 0.684               |
| 8     | **0.686**          |

**Interpr√©tation** :
- Le F1-score final de **0.686** est l√©g√®rement en dessous de l'objectif de 0.70.
- L'am√©lioration ralentit significativement apr√®s l'epoch 3.
- Le meilleur F1-score est atteint √† l'epoch 8 : **0.686**.

## üéØ R√©sultats finaux

### M√©triques principales

- **Meilleur F1-Score (validation)** : **0.686** (68.6%)
- **Accuracy finale (validation)** : **70.1%**
- **Training Loss finale** : 0.292
- **Validation Loss finale** : 0.770

### Comparaison avec les objectifs

| M√©trique | Objectif | Atteint | Statut |
|----------|----------|---------|--------|
| Accuracy | > 75% | 70.1% | ‚ö†Ô∏è En dessous |
| F1-Score macro | > 0.70 | 0.686 | ‚ö†Ô∏è L√©g√®rement en dessous |
| F1-Score par classe | > 0.65 | √Ä v√©rifier | ‚è≥ √Ä analyser |

## üîç Analyse des performances

### Points positifs ‚úÖ

1. **Am√©lioration constante** : Le mod√®le s'am√©liore r√©guli√®rement sur les m√©triques d'accuracy et F1-score.
2. **Convergence** : Le mod√®le converge vers une solution stable.
3. **Performance acceptable** : 70.1% d'accuracy et 68.6% de F1-score sont des r√©sultats raisonnables pour une classification binaire de sentiment.

### Points d'am√©lioration ‚ö†Ô∏è

1. **Overfitting** : 
   - La validation loss augmente apr√®s l'epoch 2-3.
   - L'√©cart entre training loss et validation loss s'agrandit.
   - **Recommandation** : Utiliser early stopping ou r√©gularisation plus forte.

2. **Performance sous les objectifs** :
   - L'accuracy (70.1%) est en dessous de l'objectif de 75%.
   - Le F1-score (0.686) est l√©g√®rement en dessous de 0.70.
   - **Recommandation** : Tester diff√©rents hyperparam√®tres, augmenter la taille du dataset, ou essayer data augmentation.

3. **Optimisation des epochs** :
   - Le meilleur mod√®le aurait pu √™tre obtenu plus t√¥t (epoch 3).
   - **Recommandation** : Impl√©menter early stopping bas√© sur validation loss.

## üìä Visualisations disponibles

Les fichiers suivants sont disponibles dans ce dossier :

- **`training_curves.png`** : Courbes d'√©volution de la loss, accuracy et F1-score
- **`confusion_matrix.png`** : Matrice de confusion sur le test set
- **`training_metrics.json`** : M√©triques d√©taill√©es au format JSON

## üîß Recommandations pour am√©liorer les performances

### 1. Gestion de l'overfitting

- **Early stopping** : Arr√™ter l'entra√Ænement quand la validation loss cesse de diminuer.
- **Dropout** : Augmenter le taux de dropout (actuellement 0.1).
- **Weight decay** : Augmenter le weight decay pour plus de r√©gularisation.
- **Data augmentation** : Paraphrase, back-translation, ou synonym replacement.

### 2. Optimisation des hyperparam√®tres

- **Learning rate** : Tester des learning rates plus faibles (1e-5) ou utiliser un scheduler adaptatif.
- **Batch size** : Tester diff√©rentes tailles de batch.
- **Max length** : Analyser si 128 tokens est optimal pour les tweets.

### 3. Am√©lioration des donn√©es

- **Plus de donn√©es** : Utiliser un √©chantillon plus large du dataset complet.
- **√âquilibrage des classes** : V√©rifier et corriger le d√©s√©quilibre si pr√©sent.
- **Nettoyage** : Am√©liorer le preprocessing des textes.

### 4. Architecture du mod√®le

- **Mod√®le plus grand** : Tester `roberta-large` si les ressources le permettent.
- **Mod√®le sp√©cialis√©** : Utiliser `cardiffnlp/twitter-roberta-base-sentiment` qui est pr√©-entra√Æn√© sur Twitter.

## üìÅ Fichiers de r√©sultats

### Mod√®les sauvegard√©s

- **`../models/best_model/`** : Meilleur mod√®le bas√© sur validation F1-score
- **`../models/final_model/`** : Mod√®le final apr√®s 8 epochs
- **`../models/label_mapping.json`** : Mapping des labels (Negative: 0, Positive: 1)

### Splits de donn√©es

- **`train_split.csv`** : Dataset d'entra√Ænement (~397k tweets)
- **`val_split.csv`** : Dataset de validation
- **`test_split.csv`** : Dataset de test

## üéì Conclusion

Le mod√®le RoBERTa fine-tun√© atteint des performances **acceptables** avec :
- **70.1% d'accuracy**
- **68.6% de F1-score macro**

Cependant, il y a des signes d'**overfitting** et les performances sont **l√©g√®rement en dessous des objectifs**. Les recommandations ci-dessus peuvent aider √† am√©liorer les r√©sultats.

Le mod√®le est **utilisable en production** pour une classification basique de sentiment, mais des am√©liorations sont possibles avec les optimisations sugg√©r√©es.

---

**Date de g√©n√©ration** : R√©sultats obtenus apr√®s l'ex√©cution de `roberta-1-1.ipynb`  
**Mod√®le de base** : RoBERTa (RobertaForSequenceClassification)  
**Configuration** : Voir `../models/best_model/config.json`

